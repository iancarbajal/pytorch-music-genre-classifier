{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchaudio import transforms\n",
    "from typing import Tuple, Dict, List,Optional\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently using: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nai/miniconda3/envs/pytorch-music-genre-classifier/lib/python3.13/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" # Use NVIDIA GPU (if available)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\" # Use Apple Silicon GPU (if available)\n",
    "else:\n",
    "    device = \"cpu\" # Default to CPU if no GPU is available\n",
    "\n",
    "print(f\"Currently using: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function to find classes in target directory\n",
    "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
    "    \"\"\"Finds the class folder names in a target directory.\n",
    "    \n",
    "    Assumes target directory is in standard classification format.\n",
    "\n",
    "    Args:\n",
    "        directory (str): target directory to load classnames from.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n",
    "    \n",
    "    Example:\n",
    "        find_classes(\"genres\")\n",
    "        >>> ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n",
    "    \"\"\"\n",
    "    # 1. Get the class names by scanning the target directory\n",
    "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "    \n",
    "    # 2. Raise an error if class names not found\n",
    "    if not classes:\n",
    "        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n",
    "        \n",
    "    # 3. Create a dictionary of index labels (computers prefer numerical rather than string labels)\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['blues', 'classical', 'metal', 'pop'],\n",
       " {'blues': 0, 'classical': 1, 'metal': 2, 'pop': 3})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_classes(\"genres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Subclass torch.utils.data.Dataset\n",
    "class AudioFolder(Dataset):\n",
    "    \n",
    "    # 2. Initialize with a targ_dir and transform (optional) parameter\n",
    "    def __init__(self, targ_dir: str, transform=None) -> None:\n",
    "        \n",
    "        # 3. Create class attributes\n",
    "        # Get all songs paths\n",
    "        self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.wav\"))\n",
    "        # Setup transforms\n",
    "        self.transform = transform\n",
    "        # Create classes and class_to_idx attributes\n",
    "        self.classes, self.class_to_idx = find_classes(targ_dir)\n",
    "    \n",
    "    # 4. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of samples.\"\n",
    "        return len(self.paths)\n",
    "    \n",
    "    # 5. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "        sng , sr = torchaudio.load_with_torchcodec(self.paths[index])\n",
    "        if sr <= 22050:\n",
    "            sng=sng[:11025]\n",
    "        sng=(sng-sng.mean())/sng.std()\n",
    "        class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/song.wav\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "        # Transform if necessary\n",
    "        if self.transform:\n",
    "            return self.transform(sng), class_idx # return data, label (X, y)\n",
    "        else:\n",
    "            return sng, class_idx # return data, label (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "STFT = transforms.Spectrogram(n_fft=256,hop_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = AudioFolder(targ_dir=\"genres\",transform=STFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Take in a Dataset as well as a list of class names\n",
    "def display_random_images(dataset: torch.utils.data.dataset.Dataset,\n",
    "                          classes: List[str] = None,\n",
    "                          n: int = 10,\n",
    "                          display_shape: bool = True,\n",
    "                          seed: int = None):\n",
    "    \n",
    "    # 2. Adjust display if n too high\n",
    "    if n > 10:\n",
    "        n = 10\n",
    "        display_shape = False\n",
    "        print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")\n",
    "    \n",
    "    # 3. Set random seed\n",
    "    if seed:\n",
    "        random.seed(seed)\n",
    "\n",
    "    # 4. Get random sample indexes\n",
    "    random_samples_idx = random.sample(range(len(dataset)), k=n)\n",
    "\n",
    "    # 5. Setup plot\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    # 6. Loop through samples and display random samples \n",
    "    for i, targ_sample in enumerate(random_samples_idx):\n",
    "        targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]\n",
    "\n",
    "        # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -> [color_channels, height, width]\n",
    "        targ_image_adjust = targ_image.permute(1, 2, 0)\n",
    "\n",
    "        # Plot adjusted samples\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.specgram(targ_image_adjust,Fs=11025)\n",
    "        plt.axis(\"off\")\n",
    "        if classes:\n",
    "            title = f\"class: {classes[targ_label]}\"\n",
    "            if display_shape:\n",
    "                title = title #+ f\"\\nshape: {targ_image_adjust.shape}\"\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68247/2924035272.py:33: UserWarning: Only one segment is calculated since parameter NFFT (=256) >= signal length (=129).\n",
      "  plt.specgram(targ_image_adjust,Fs=11025)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_names= data.classes\n",
    "display_random_images(data, \n",
    "                      n=5, \n",
    "                      classes=class_names,\n",
    "                      seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#80% train 20% test split\n",
    "train, test= random_split(data,[320,80])\n",
    "train_dataset=train.dataset\n",
    "test_dataset=test.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "train_dataloader = DataLoader(dataset=train_dataset, # use custom created train Dataset\n",
    "                                     batch_size=BATCH_SIZE, # how many samples per batch?\n",
    "                                     num_workers=os.cpu_count(), # how many subprocesses to use for data loading? (higher = more)\n",
    "                                     shuffle=True) # shuffle the data?\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset, # use custom created train Dataset\n",
    "                                     batch_size=BATCH_SIZE, # how many samples per batch?\n",
    "                                     num_workers=os.cpu_count(), # how many subprocesses to use for data loading? (higher = more)\n",
    "                                     shuffle=True) # shuffle the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STFTCNNBlock(nn.Module):\n",
    "    \"\"\"A single convolutional block for STFT CNN\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, \n",
    "                 kernel_size: int, pooling_type: str = 'max'):\n",
    "        super(STFTCNNBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, \n",
    "                             kernel_size=kernel_size, padding='same')\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        if pooling_type == 'max':\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        else:  # average pooling\n",
    "            self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STFTCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN for processing STFT features as described in the paper.\n",
    "    Architecture: 5 convolutional blocks + fully connected layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels: int = 1, \n",
    "                 conv_configs: Optional[list] = None,\n",
    "                 num_classes: int = 10, \n",
    "                 feature_dim: int = 200):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_channels: Number of input channels (1 for STFT magnitude)\n",
    "            conv_configs: List of tuples (out_channels, kernel_size, pooling_type)\n",
    "                         If None, uses the configuration from Table 1 in the paper\n",
    "            num_classes: Number of music genres\n",
    "            feature_dim: Dimension of extracted features (200 as in paper)\n",
    "        \"\"\"\n",
    "        super(STFTCNN, self).__init__()\n",
    "        \n",
    "        # Default configuration based on Table 1 (STFT CNN column)\n",
    "        if conv_configs is None:\n",
    "            conv_configs = [\n",
    "                (31, 15, 'max'),      # Conv1: 31 filters, 15x15, MaxPool\n",
    "                (38, 11, 'max'),      # Conv2: 38 filters, 11x11, MaxPool  \n",
    "                (43, 9, 'max'),       # Conv3: 43 filters, 9x9, MaxPool\n",
    "                (57, 6, 'average'),   # Conv4: 57 filters, 6x6, AvgPool\n",
    "                (64, 3, 'average')    # Conv5: 64 filters, 3x3, AvgPool\n",
    "            ]\n",
    "        \n",
    "        self.conv_blocks = nn.ModuleList()\n",
    "        in_channels = input_channels\n",
    "        \n",
    "        # Create 5 convolutional blocks\n",
    "        for out_channels, kernel_size, pooling_type in conv_configs:\n",
    "            block = STFTCNNBlock(in_channels, out_channels, \n",
    "                               kernel_size, pooling_type)\n",
    "            self.conv_blocks.append(block)\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        # Placeholder for adaptive pooling and fully connected layer\n",
    "        # These will be initialized after the first forward pass\n",
    "        self.adaptive_pool = None\n",
    "        self.fc = None\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def _init_fc_layer(self, x):\n",
    "        \"\"\"Initialize the fully connected layer based on feature map size\"\"\"\n",
    "        # Use adaptive average pooling to get fixed size output\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        pooled = self.adaptive_pool(x)\n",
    "        \n",
    "        # Calculate flattened feature size\n",
    "        fc_input_size = pooled.numel() // pooled.shape[0]\n",
    "        \n",
    "        # Create fully connected layer for feature extraction\n",
    "        self.fc = nn.Linear(fc_input_size, self.feature_dim)\n",
    "        \n",
    "    def forward(self, x, return_features: bool = False):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        \n",
    "        Args:\n",
    "            x: Input STFT tensor of shape (batch_size, 1, height, width)\n",
    "            return_features: If True, return features instead of classification\n",
    "        \n",
    "        Returns:\n",
    "            features or logits depending on return_features\n",
    "        \"\"\"\n",
    "        # Pass through convolutional blocks\n",
    "        for block in self.conv_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Initialize FC layer on first forward pass\n",
    "        if self.fc is None:\n",
    "            self._init_fc_layer(x)\n",
    "            self.fc = self.fc.to(x.device)\n",
    "            self.adaptive_pool = self.adaptive_pool.to(x.device)\n",
    "        \n",
    "        # Global pooling and flatten\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.fc(x)\n",
    "        \n",
    "        if return_features:\n",
    "            return features\n",
    "        else:\n",
    "            # Add classification layer if needed\n",
    "            if not hasattr(self, 'classifier'):\n",
    "                self.classifier = nn.Linear(self.feature_dim, self.num_classes)\n",
    "                self.classifier = self.classifier.to(features.device)\n",
    "            \n",
    "            return self.classifier(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = STFTCNN(input_channels=1, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: torch.Size([1, 200])\n",
      "Logits shape: torch.Size([1, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nai/miniconda3/envs/pytorch-music-genre-classifier/lib/python3.13/site-packages/torch/nn/modules/conv.py:543: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /pytorch/aten/src/ATen/native/Convolution.cpp:1027.)\n",
      "  return F.conv2d(\n"
     ]
    }
   ],
   "source": [
    "tinput, tlabel= next(iter(train_dataloader))\n",
    "\n",
    "features = model(tinput, return_features=True)\n",
    "print(f\"Feature shape: {features.shape}\")\n",
    "\n",
    "# Forward pass for classification\n",
    "logits = model(tinput, return_features=False)\n",
    "print(f\"Logits shape: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6214,  2.4725, -2.0031,  2.8918]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "STFTCNN                                  [1, 4]                    --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─STFTCNNBlock: 2-1                 [1, 31, 64, 646]          --\n",
       "│    │    └─Conv2d: 3-1                  [1, 31, 129, 1293]        7,006\n",
       "│    │    └─ReLU: 3-2                    [1, 31, 129, 1293]        --\n",
       "│    │    └─MaxPool2d: 3-3               [1, 31, 64, 646]          --\n",
       "│    └─STFTCNNBlock: 2-2                 [1, 38, 32, 323]          --\n",
       "│    │    └─Conv2d: 3-4                  [1, 38, 64, 646]          142,576\n",
       "│    │    └─ReLU: 3-5                    [1, 38, 64, 646]          --\n",
       "│    │    └─MaxPool2d: 3-6               [1, 38, 32, 323]          --\n",
       "│    └─STFTCNNBlock: 2-3                 [1, 43, 16, 161]          --\n",
       "│    │    └─Conv2d: 3-7                  [1, 43, 32, 323]          132,397\n",
       "│    │    └─ReLU: 3-8                    [1, 43, 32, 323]          --\n",
       "│    │    └─MaxPool2d: 3-9               [1, 43, 16, 161]          --\n",
       "│    └─STFTCNNBlock: 2-4                 [1, 57, 8, 80]            --\n",
       "│    │    └─Conv2d: 3-10                 [1, 57, 16, 161]          88,293\n",
       "│    │    └─ReLU: 3-11                   [1, 57, 16, 161]          --\n",
       "│    │    └─AvgPool2d: 3-12              [1, 57, 8, 80]            --\n",
       "│    └─STFTCNNBlock: 2-5                 [1, 64, 4, 40]            --\n",
       "│    │    └─Conv2d: 3-13                 [1, 64, 8, 80]            32,896\n",
       "│    │    └─ReLU: 3-14                   [1, 64, 8, 80]            --\n",
       "│    │    └─AvgPool2d: 3-15              [1, 64, 4, 40]            --\n",
       "├─AdaptiveAvgPool2d: 1-2                 [1, 64, 4, 4]             --\n",
       "├─Linear: 1-3                            [1, 200]                  205,000\n",
       "├─Linear: 1-4                            [1, 4]                    804\n",
       "==========================================================================================\n",
       "Total params: 608,972\n",
       "Trainable params: 608,972\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 8.68\n",
       "==========================================================================================\n",
       "Input size (MB): 0.67\n",
       "Forward/backward pass size (MB): 58.99\n",
       "Params size (MB): 2.44\n",
       "Estimated Total Size (MB): 62.10\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install torchinfo if it's not available, import it if it is\n",
    "try: \n",
    "    import torchinfo\n",
    "except:\n",
    "    !pip install torchinfo\n",
    "    import torchinfo\n",
    "    \n",
    "from torchinfo import summary\n",
    "summary(model, input_size=tinput.shape) # do a test pass through of an example input size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metrics across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module):\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "    \n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "    \n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "            \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. Take in various parameters required for training and test steps\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 5):\n",
    "    \n",
    "    # 2. Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "    \n",
    "    # 3. Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn)\n",
    "        \n",
    "        # 4. Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # 5. Update results dictionary\n",
    "        # Ensure all data is moved to CPU and converted to float for storage\n",
    "        results[\"train_loss\"].append(train_loss.item() if isinstance(train_loss, torch.Tensor) else train_loss)\n",
    "        results[\"train_acc\"].append(train_acc.item() if isinstance(train_acc, torch.Tensor) else train_acc)\n",
    "        results[\"test_loss\"].append(test_loss.item() if isinstance(test_loss, torch.Tensor) else test_loss)\n",
    "        results[\"test_acc\"].append(test_acc.item() if isinstance(test_acc, torch.Tensor) else test_acc)\n",
    "\n",
    "    # 6. Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [01:13<04:52, 73.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.8207 | train_acc: 0.3950 | test_loss: 0.6031 | test_acc: 0.7650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [02:27<03:41, 73.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 0.8684 | train_acc: 0.6850 | test_loss: 0.4166 | test_acc: 0.8525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [03:42<02:28, 74.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss: 0.4276 | train_acc: 0.8600 | test_loss: 0.6048 | test_acc: 0.8075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [04:57<01:14, 74.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss: 0.4923 | train_acc: 0.8375 | test_loss: 0.3001 | test_acc: 0.8675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [06:13<00:00, 74.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss: 0.2358 | train_acc: 0.9150 | test_loss: 0.3090 | test_acc: 0.8825\n",
      "Total training time: 373.114 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(42) \n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Recreate an instance of TinyVGG\n",
    "model_0 = STFTCNN()\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer()\n",
    "\n",
    "# Train model_0 \n",
    "model_0_results = train(model=model_0, \n",
    "                        train_dataloader=train_dataloader,\n",
    "                        test_dataloader=test_dataloader,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn, \n",
    "                        epochs=NUM_EPOCHS)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-music-genre-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
